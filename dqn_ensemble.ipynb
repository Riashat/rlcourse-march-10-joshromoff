{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__DQN vs DQN-ensemble on Catch__\n",
    "\n",
    "Background: Deep ensemble learning was looked at in Osband et al. 2016 (https://arxiv.org/pdf/1602.04621.pdf). The main motivation behind learning several randomly initialized Q functions is to drive exploration. In general, each Q function could be defined by a unique set of parameters. However, it can be more efficient to share the lower levels of the network (convnet), and to have each Q function learn one extra unique linear layer on top of the shared representation (referenced as a \"head\"). Whereas Osband et al. 2016 propose a more general algorithm (see Appendix B, Algorithm 1), this presentation focusses on the sub-case where the agent acts based on the action with the maximum number of votes; where a single vote represents the action with the max q value for a single \"head\". \n",
    "\n",
    "__Implementation Details__:\n",
    "\n",
    "_Domain_: \n",
    "\n",
    "The total screen size is 10x10, the agent has 3 actions [left, no-op, right] - controls a basket of three pixles and attempts to catch a falling fruit of a single pixle. The code can be found in \"catch.py\", it was taken from https://gist.github.com/EderSantana/c7222daa328f0e885093.\n",
    "\n",
    "_Learning_:\n",
    "\n",
    "No target network is used. There are two main reasons for this reduction; the domain itself is fairly simple and clean (compared to atari), and to highlight the fact that learning an ensemble of randomly initialized Q functions can increase stability.\n",
    "\n",
    "The agents each do Q learning updates on both their unique parameters (Q heads) and the shared ones, but the loss is normalized by the number of agents. This is similar to what Osband et al. 2016 did with their Atari experiments. \n",
    "\n",
    "Epsilon greedy exploration is used with epsilon = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A really simple replay buffer class\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Replay(object):\n",
    "    def __init__(self, max_size, state_shape):\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "        self.state_shape = state_shape\n",
    "        self.s = np.zeros([max_size] + state_shape, dtype='float32')\n",
    "        self.a = np.zeros(max_size, dtype='int32')\n",
    "        self.r = np.zeros(max_size, dtype='float32')\n",
    "        self.s2 = np.zeros([max_size] + state_shape, dtype='float32')\n",
    "        self.t = np.zeros(max_size, dtype='float32')\n",
    "        \n",
    "    def sample_batch(self, batch_size):\n",
    "        indices = np.random.randint(low=0, high=self.size, size=batch_size)\n",
    "        return self.s[indices], self.a[indices], self.r[indices], self.s2[indices], self.t[indices]\n",
    "    \n",
    "    def add_transition(self, s, a, r, s2, t):\n",
    "        s = s.reshape(self.state_shape)\n",
    "        s2 = s2.reshape(self.state_shape)\n",
    "        self.s[self.index % self.max_size] = s\n",
    "        self.a[self.index % self.max_size] = a\n",
    "        self.r[self.index % self.max_size] = r\n",
    "        self.s2[self.index % self.max_size] = s2\n",
    "        self.t[self.index % self.max_size] = t\n",
    "        self.index += 1\n",
    "        self.size = min(self.index, self.max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GPU (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Neural network based agent - requires Keras and Theano. \n",
    "    Note: is not compatible with tensorflow due to the differing dimensions\n",
    "          - be sure to set the keras backend to 'theano' and the dim-ordering to 'th'\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.core import Dense, Flatten, Reshape\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "import theano\n",
    "\n",
    "\n",
    "def build_network(state_shape, nb_actions, nb_agents):\n",
    "    # shared layers\n",
    "    input_dim = tuple(state_shape)\n",
    "    states = Input(shape=input_dim, dtype='float32', name='states')\n",
    "    shared_conv = Convolution2D(nb_filter=16,\n",
    "                                nb_row=3,\n",
    "                                nb_col=3,\n",
    "                                border_mode='same',\n",
    "                                subsample=(1, 1),\n",
    "                                activation='relu',\n",
    "                                init='he_uniform')(states)\n",
    "    flatten = Flatten()(shared_conv)\n",
    "    shared_dense = Dense(output_dim=128, init='he_uniform', activation='relu')(flatten)\n",
    "    \n",
    "    # unique q-heads\n",
    "    q_heads = [Dense(output_dim=nb_actions, init='he_uniform', activation='linear')(shared_dense)\n",
    "               for i in range(nb_agents)]\n",
    "    \n",
    "    # merge q-heads and reshape\n",
    "    merged = merge(q_heads, mode='concat') if nb_agents > 1 else q_heads[0]\n",
    "    out = Reshape((nb_agents, nb_actions))(merged)\n",
    "    \n",
    "    return Model(input=states, output=out)\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, state_shape, nb_actions, nb_agents, gamma=0.99, lr=0.001, batch_size=32):\n",
    "        self.state_shape = state_shape\n",
    "        self.nb_actions = nb_actions\n",
    "        self.nb_agents = nb_agents\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = 32\n",
    "        self.replay = Replay(max_size=10000, state_shape=state_shape)\n",
    "        self.network = build_network(state_shape, nb_actions, nb_agents)\n",
    "        self._compile(state_shape, nb_actions, nb_agents)\n",
    "\n",
    "    def _compile(self, state_shape, nb_actions, nb_agents):\n",
    "        # Inputs\n",
    "        s = K.placeholder(shape=tuple([None] + state_shape))\n",
    "        a = K.placeholder(ndim=1, dtype='int32')\n",
    "        r = K.placeholder(ndim=2, dtype='float32')\n",
    "        s2 = K.placeholder(shape=tuple([None] + state_shape))\n",
    "        t = K.placeholder(ndim=2, dtype='float32')\n",
    "\n",
    "        # Q(s, a) for each agent\n",
    "        q = self.network(s) # q has shape (batch_size, nb_agents, nb_actions)\n",
    "        preds = q[K.T.arange(s.shape[0]), :, a]\n",
    "        \n",
    "        # One hot vector representing the max Q(s, a) for each agent: \n",
    "        # eg. Q(s) = (0.5, 0.25, 0.1) -> (1, 0, 0)\n",
    "        votes = K.sum(K.eye(self.nb_actions, dtype='float32')[K.argmax(q[0], axis=1)], axis=0)\n",
    "\n",
    "        # r + (1 - t) * gamma * max_a(Q'(s')) for each agent\n",
    "        q2 = theano.gradient.disconnected_grad(self.network(s2))\n",
    "        q2_max = K.max(q2, axis=2)\n",
    "        targets = r + (K.cast_to_floatx(1) - t) * self.gamma * q2_max\n",
    "\n",
    "        # Loss and Updates\n",
    "        cost = K.sum(0.5 * (targets - preds) ** 2) \n",
    "        optimizer = RMSprop(lr=self.lr, rho=.95, epsilon=1e-7)\n",
    "        updates = optimizer.get_updates(params=self.network.trainable_weights, loss=cost, constraints={})\n",
    "\n",
    "        # Compiled Functions\n",
    "        self._train = K.function(inputs=[s, a, r, s2, t], outputs=[cost], updates=updates)\n",
    "        self._predict = K.function(inputs=[s], outputs=votes)\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        state = np.expand_dims(state.reshape(self.state_shape), axis=0)\n",
    "        if np.random.binomial(1, epsilon):\n",
    "            return np.random.randint(self.nb_actions)\n",
    "        else:\n",
    "            votes = self._predict([state])\n",
    "            return np.argmax(votes)\n",
    "        \n",
    "    def learn(self):\n",
    "        s, a, r, s2, t = self.replay.sample_batch(self.batch_size)\n",
    "        r = np.expand_dims(r, axis=1)\n",
    "        t = np.expand_dims(t, axis=1)\n",
    "        r = np.tile(r, reps=(1, self.nb_agents))\n",
    "        t = np.tile(t, reps=(1, self.nb_agents))\n",
    "        return self._train([s, a, r, s2, t])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.network = build_network(self.state_shape, self.nb_actions, self.nb_agents)\n",
    "        self._compile(self.state_shape, self.nb_actions, self.nb_agents)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Global imports and variables\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from catch import Catch\n",
    "\n",
    "nb_runs_each = 3\n",
    "nb_episodes = 50\n",
    "nb_heads_list = [1, 2, 5, 10]\n",
    "min_replay_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Runs each agent defined by the number of heads it uses (\"nb_heads_list\") -\n",
    "    for a fixed number of episodes (\"nb_episodes\"). \n",
    "    Each run is repeated \"nb_runs_each\" times to compute some measure of variance.\n",
    "\"\"\"\n",
    "\n",
    "env = Catch()\n",
    "agents = [Agent(env.state_shape, env.nb_actions, nb_heads) for nb_heads in nb_heads_list]\n",
    "all_agent_rewards = np.empty((nb_runs_each, nb_episodes, len(nb_heads_list)))\n",
    "\n",
    "for agent_index, agent in enumerate(agents):\n",
    "    for run in range(nb_runs_each):\n",
    "        rewards = 0\n",
    "        for ep in range(nb_episodes):\n",
    "            env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                s = env.observe()\n",
    "                a = agent.get_action(s, epsilon=0.1)\n",
    "                s2, r, terminal = env.act(a)\n",
    "                agent.replay.add_transition(s, a, r, s2, terminal)\n",
    "                if agent.replay.size > min_replay_size:\n",
    "                    agent.learn()\n",
    "                rewards += r\n",
    "            all_agent_rewards[run, ep, agent_index] = rewards\n",
    "        agent.reset()\n",
    "            \n",
    "with open('rewards', 'wb') as f:\n",
    "    pickle.dump(all_agent_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('rewards', 'rb') as f:\n",
    "     all_agent_rewards = pickle.load(f)\n",
    "\n",
    "plot = seaborn.tsplot(data=all_agent_rewards, legend=True, condition=nb_heads_list)\n",
    "plot.legend(loc='upper left', title='# of heads')\n",
    "plot.set_ylabel(\"Cummulative Reward\")\n",
    "plot.set_xlabel(\"Number of Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
